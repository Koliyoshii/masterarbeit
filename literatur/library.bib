@article{platinsky,
   abstract = {In this paper we present the first published end-to-end production computer-vision system for powering city-scale shared augmented reality experiences on mobile devices. In doing so we propose a new formulation for an experience-based mapping framework as an effective solution to the key issues of city-scale SLAM scalability, robustness, map updates and all-time all-weather performance required by a production system. Furthermore, we propose an effective way of synchronising SLAM systems to deliver seamless real-time localisation of multiple edge devices at the same time. All this in the presence of network latency and bandwidth limitations. The resulting system is deployed and tested at scale in San Francisco where it delivers AR experiences in a mapped area of several hundred kilometers. To foster further development of this area we offer the data set to the public, constituting the largest of this kind to date.},
   author = {Lukas Platinsky and Michal Szabados and Filip Hlasek and Ross Hemsley and Luca Del Pero and Andrej Pancik and Bryan Baum and Hugo Grimmett and Peter Ondruska},
   month = {11},
   title = {Collaborative Augmented Reality on Smartphones via Life-long City-scale Maps},
   url = {http://arxiv.org/abs/2011.05370},
   year = {2020},
}
@book{doerner,
   author = {Ralf' 'DÃ¶rner and Wolfgang' 'Broll and Paul' 'Grimm and Bernhard' 'Jung},
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-662-58861-1},
   isbn = {978-3-662-58860-4},
   publisher = {Springer Berlin Heidelberg},
   title = {Virtual und Augmented Reality (VR/AR)},
   volume = {2},
   year = {2019},
}
@book{hartleyzisserman,
   author = {Richard Hartley and Andrew Zisserman},
   doi = {10.1017/CBO9780511811685},
   isbn = {9780521540513},
   month = {3},
   publisher = {Cambridge University Press},
   title = {Multiple View Geometry in Computer Vision},
   volume = {2},
   year = {2004},
}
@inproceedings{lowe1999,
   author = {David G. Lowe},
   doi = {10.1109/ICCV.1999.790410},
   isbn = {0-7695-0164-8},
   journal = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
   pages = {1150-1157 vol.2},
   publisher = {IEEE},
   title = {Object recognition from local scale-invariant features},
   year = {1999},
}
@article{bay2008,
   author = {Herbert Bay and Andreas Ess and Tinne Tuytelaars and Luc Van Gool},
   doi = {https://doi.org/10.1016/j.cviu.2007.09.014},
   issn = {10773142},
   issue = {3},
   journal = {Computer Vision and Image Understanding},
   month = {6},
   pages = {346-359},
   title = {Speeded-Up Robust Features (SURF)},
   volume = {110},
   year = {2008},
}
@article{slam1,
   author = {H. Durrant-Whyte and T. Bailey},
   doi = {10.1109/MRA.2006.1638022},
   issn = {1070-9932},
   issue = {2},
   journal = {IEEE Robotics and Automation Magazine},
   month = {6},
   pages = {99-110},
   title = {Simultaneous localization and mapping: part I},
   volume = {13},
   url = {https://ieeexplore.ieee.org/document/1638022/},
   year = {2006},
}
@article{slam2,
   author = {T. Bailey and H. Durrant-Whyte},
   doi = {10.1109/MRA.2006.1678144},
   issn = {1070-9932},
   issue = {3},
   journal = {IEEE Robotics and Automation Magazine},
   month = {9},
   pages = {108-117},
   title = {Simultaneous localization and mapping (SLAM): part II},
   volume = {13},
   year = {2006},
}
@inproceedings{Sudirman,
   author = {Sud Sudirman and Abdennour El-Rhalibi},
   doi = {10.1109/CIT/IUCC/DASC/PICOM.2015.150},
   isbn = {978-1-5090-0154-5},
   journal = {2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing},
   month = {10},
   pages = {994-999},
   publisher = {IEEE},
   title = {Improving Camera Pose Estimation for Indoor Marker-less Augmented Reality},
   year = {2015},
}
@article{Santi,
   abstract = {<p>Augmented Reality (AR) is worldwide recognized as one of the leading technologies of the 21st century and one of the pillars of the new industrial revolution envisaged by the Industry 4.0 international program. Several papers describe, in detail, specific applications of Augmented Reality developed to test its potentiality in a variety of fields. However, there is a lack of sources detailing the current limits of this technology in the event of its introduction in a real working environment where everyday tasks could be carried out by operators using an AR-based approach. A literature analysis to detect AR strength and weakness has been carried out, and a set of case studies has been implemented by authors to find the limits of current AR technologies in industrial applications outside the laboratory-protected environment. The outcome of this paper is that, even though Augmented Reality is a well-consolidated computer graphic technique in research applications, several improvements both from a software and hardware point of view should be introduced before its introduction in industrial operations. The originality of this paper lies in the detection of guidelines to improve the Augmented Reality potentialities in factories and industries.</p>},
   author = {Gian Maria Santi and Alessandro Ceruti and Alfredo Liverani and Francesco Osti},
   doi = {10.3390/technologies9020033},
   issn = {2227-7080},
   issue = {2},
   journal = {Technologies},
   month = {4},
   pages = {33},
   title = {Augmented Reality in Industry 4.0 and Future Innovation Programs},
   volume = {9},
   year = {2021},
}
@article{Huang,
   abstract = {<p>Due to the popularity of indoor positioning technology, indoor navigation applications have been deployed in large buildings, such as hospitals, airports, and train stations, to guide visitors to their destinations. A commonly-used user interface, shown on smartphones, is a 2D floor map with a route to the destination. The navigation instructions, such as turn left, turn right, and go straight, pop up on the screen when users come to an intersection. However, owing to the restrictions of a 2D navigation map, users may face mental pressure and get confused while they are making a connection between the real environment and the 2D navigation map before moving forward. For this reason, we developed ARBIN, an augmented reality-based navigation system, which posts navigation instructions on the screen of real-world environments for ease of use. Thus, there is no need for users to make a connection between the navigation instructions and the real-world environment. In order to evaluate the applicability of ARBIN, a series of experiments were conducted in the outpatient area of the National Taiwan University Hospital YunLin Branch, which is nearly 1800 m2, with 35 destinations and points of interests, such as a cardiovascular clinic, x-ray examination room, pharmacy, and so on. Four different types of smartphone were adopted for evaluation. Our results show that ARBIN can achieve 3 to 5 m accuracy, and provide users with correct instructions on their way to the destinations. ARBIN proved to be a practical solution for indoor navigation, especially for large buildings.</p>},
   author = {Bo-Chen Huang and Jiun Hsu and Edward T.-H. Chu and Hui-Mei Wu},
   doi = {10.3390/s20205890},
   issn = {1424-8220},
   issue = {20},
   journal = {Sensors},
   month = {10},
   pages = {5890},
   title = {ARBIN: Augmented Reality Based Indoor Navigation System},
   volume = {20},
   year = {2020},
}
@inproceedings{Liu2020,
   author = {Weiquan Liu and Baiqi Lai and Cheng Wang and Xuesheng Bian and Wentao Yang and Yan Xia and Xiuhong Lin and Shang-Hong Lai and Dongdong Weng and Jonathan Li},
   doi = {10.1109/VRW50115.2020.00178},
   isbn = {978-1-7281-6532-5},
   journal = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
   month = {3},
   pages = {654-655},
   publisher = {IEEE},
   title = {Learning to Match 2D Images and 3D LiDAR Point Clouds for Outdoor Augmented Reality},
   year = {2020},
}
@article{Furukawa2011,
   abstract = {<p>We present a system that can reconstruct 3D geometry from large, unorganized collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo-sharing sites. Our system is built on a set of new, distributed computer vision algorithms for image matching and 3D reconstruction, designed to maximize parallelism at each stage of the pipeline and to scale gracefully with both the size of the problem and the amount of available computation. Our experimental results demonstrate that it is now possible to reconstruct city-scale image collections with more than a hundred thousand images in less than a day.</p>},
   author = {Sameer Agarwal and Yasutaka Furukawa and Noah Snavely and Ian Simon and Brian Curless and Steven M. Seitz and Richard Szeliski},
   doi = {10.1145/2001269.2001293},
   issn = {0001-0782},
   issue = {10},
   journal = {Communications of the ACM},
   month = {10},
   pages = {105-112},
   title = {Building Rome in a day},
   volume = {54},
   year = {2011},
}
@article{1981,
   abstract = {<p>A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing</p>},
   author = {Martin A. Fischler and Robert C. Bolles},
   doi = {10.1145/358669.358692},
   issn = {0001-0782},
   issue = {6},
   journal = {Communications of the ACM},
   month = {6},
   pages = {381-395},
   title = {Random sample consensus},
   volume = {24},
   year = {1981},
}
@booksection{Agarwal2010,
   author = {Sameer Agarwal and Noah Snavely and Steven M. Seitz and Richard Szeliski},
   doi = {10.1007/978-3-642-15552-9_3},
   pages = {29-42},
   title = {Bundle Adjustment in the Large},
   year = {2010},
}
@inproceedings{Seitz2006,
   author = {S.M. Seitz and B. Curless and J. Diebel and D. Scharstein and R. Szeliski},
   doi = {10.1109/CVPR.2006.19},
   isbn = {0-7695-2597-0},
   journal = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 (CVPR'06)},
   pages = {519-528},
   publisher = {IEEE},
   title = {A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms},
   year = {2006},
}
@article{Pollefeys2008,
   author = {M. Pollefeys and D. NistÃ©r and J.-M. Frahm and A. Akbarzadeh and P. Mordohai and B. Clipp and C. Engels and D. Gallup and S.-J. Kim and P. Merrell and C. Salmi and S. Sinha and B. Talton and L. Wang and Q. Yang and H. StewÃ©nius and R. Yang and G. Welch and H. Towles},
   doi = {10.1007/s11263-007-0086-4},
   issn = {0920-5691},
   issue = {2-3},
   journal = {International Journal of Computer Vision},
   month = {7},
   pages = {143-167},
   title = {Detailed Real-Time Urban 3D Reconstruction from Video},
   volume = {78},
   year = {2008},
}
@inproceedings{Schall2009,
   author = {Gerhard Schall and Daniel Wagner and Gerhard Reitmayr and Elise Taichmann and Manfred Wieser and Dieter Schmalstieg and Bernhard Hofmann-Wellenhof},
   doi = {10.1109/ISMAR.2009.5336489},
   isbn = {978-1-4244-5390-0},
   journal = {2009 8th IEEE International Symposium on Mixed and Augmented Reality},
   month = {10},
   pages = {153-162},
   publisher = {IEEE},
   title = {Global pose estimation using multi-sensor fusion for outdoor Augmented Reality},
   year = {2009},
}
@booksection{engel2014,
   author = {Jakob Engel and Thomas Schoeps and Daniel Cremers},
   doi = {10.1007/978-3-319-10605-2_54},
   pages = {834-849},
   title = {LSD-SLAM: Large-Scale Direct Monocular SLAM},
   year = {2014},
}
@inproceedings{bailey2006,
   author = {Tim Bailey and Juan Nieto and Jose Guivant and Michael Stevens and Eduardo Nebot},
   doi = {10.1109/IROS.2006.281644},
   isbn = {1-4244-0258-1},
   journal = {2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
   month = {10},
   pages = {3562-3568},
   publisher = {IEEE},
   title = {Consistency of the EKF-SLAM Algorithm},
   year = {2006},
}
@article{,
   author = {Raul Mur-Artal and J. M. M. Montiel and Juan D. Tardos},
   doi = {10.1109/TRO.2015.2463671},
   issn = {1552-3098},
   issue = {5},
   journal = {IEEE Transactions on Robotics},
   month = {10},
   pages = {1147-1163},
   title = {ORB-SLAM: A Versatile and Accurate Monocular SLAM System},
   volume = {31},
   year = {2015},
}
@inproceedings{klein2007,
   author = {Georg Klein and David Murray},
   doi = {10.1109/ISMAR.2007.4538852},
   isbn = {978-1-4244-1749-0},
   journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
   month = {11},
   pages = {1-10},
   publisher = {IEEE},
   title = {Parallel Tracking and Mapping for Small AR Workspaces},
   year = {2007},
}
@patent{esha2017,
   author = {NERURKAR ESHA and LYNEN SIMON and ZHAO SHENG},
   city = {US},
   isbn = {US 2017/0336511 A1},
   publisher = {GOOGLE INC},
   title = {SYSTEM AND METHOD FOR CONCURRENT ODOMETRY AND MAPPING},
   url = {https://lens.org/137-618-961-961-779},
   year = {2017},
}
@article{Marneanu2014,
   abstract = {<p>Augmented Reality (AR) is the evolution of the concept of Virtual Reality (VR). Its goal is to enhance a person's perception of the surrounding world. AR is a fast growing state of the art technology and a variety of implementation tools thereof exist today. Due to the heterogeneity of available technologies, the choice of the appropriate framework for a mobile application is difficult to make. These frameworks implement different tracking techniques and have to provide support to various constraints. This publication aims to point out that the choice of the appropriate framework depends on the context of the app to be developed. As expected, it is accurate that no framework is entirely the best, but rather that each exhibits strong and weak points. Our results demonstrate that given a set of constraints, one framework can outperform others. We anticipate our research to be the starting point for testing of other frameworks, given various constraints. The frameworks evaluated here are open-source or have been purchased under Academic License.</p>},
   author = {Iulia Marneanu and Martin Ebner and Thomas Roessler},
   doi = {10.3991/ijim.v8i4.3974},
   issn = {1865-7923},
   issue = {4},
   journal = {International Journal of Interactive Mobile Technologies (iJIM)},
   month = {10},
   pages = {37},
   title = {Evaluation of Augmented Reality Frameworks for Android Development},
   volume = {8},
   year = {2014},
}
@article{Cadena2016,
   author = {Cesar Cadena and Luca Carlone and Henry Carrillo and Yasir Latif and Davide Scaramuzza and Jose Neira and Ian Reid and John J. Leonard},
   doi = {10.1109/TRO.2016.2624754},
   issn = {1552-3098},
   issue = {6},
   journal = {IEEE Transactions on Robotics},
   month = {12},
   pages = {1309-1332},
   title = {Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age},
   volume = {32},
   year = {2016},
}
@article{Holynski2018,
   abstract = {<p>Current AR systems only track sparse geometric features but do not compute depth for all pixels. For this reason, most AR effects are pure overlays that can never be occluded by real objects. We present a novel algorithm that propagates sparse depth to every pixel in near realtime. The produced depth maps are spatio-temporally smooth but exhibit sharp discontinuities at depth edges. This enables AR effects that can fully interact with and be occluded by the real scene. Our algorithm uses a video and a sparse SLAM reconstruction as input. It starts by estimating soft depth edges from the gradient of optical flow fields. Because optical flow is unreliable near occlusions we compute forward and backward flow fields and fuse the resulting depth edges using a novel reliability measure. We then localize the depth edges by thinning and aligning them with image edges. Finally, we optimize the propagated depth smoothly but encourage discontinuities at the recovered depth edges. We present results for numerous real-world examples and demonstrate the effectiveness for several occlusion-aware AR video effects. To quantitatively evaluate our algorithm we characterize the properties that make depth maps desirable for AR applications, and present novel evaluation metrics that capture how well these are satisfied. Our results compare favorably to a set of competitive baseline algorithms in this context.</p>},
   author = {Aleksander Holynski and Johannes Kopf},
   doi = {10.1145/3272127.3275083},
   issn = {0730-0301},
   issue = {6},
   journal = {ACM Transactions on Graphics},
   month = {12},
   pages = {1-11},
   title = {Fast depth densification for occlusion-aware augmented reality},
   volume = {37},
   year = {2018},
}
@article{Valentin2018,
   abstract = {<p> Augmented reality (AR) for smartphones has matured from a technology for earlier adopters, available only on select high-end phones, to one that is truly available to the general public. One of the key breakthroughs has been in <italic>low-compute</italic> methods for six degree of freedom (6DoF) tracking on phones using only the <italic>existing hardware</italic> (camera and inertial sensors). 6DoF tracking is the cornerstone of smartphone AR allowing virtual content to be precisely locked on top of the real world. However, to really give users the impression of believable AR, one requires <italic>mobile depth.</italic> Without depth, even simple effects such as a virtual object being correctly occluded by the real-world is impossible. However, requiring a mobile depth sensor would severely restrict the access to such features. In this article, we provide a novel pipeline for mobile depth that supports a wide array of mobile phones, and uses only the existing monocular color sensor. Through several technical contributions, we provide the ability to compute low latency dense depth maps using only a single CPU core of a wide range of (medium-high) mobile phones. We demonstrate the capabilities of our approach on high-level AR applications including real-time navigation and shopping. </p>},
   author = {Julien Valentin and Adarsh Kowdle and Jonathan T. Barron and Neal Wadhwa and Max Dzitsiuk and Michael Schoenberg and Vivek Verma and Ambrus Csaszar and Eric Turner and Ivan Dryanovski and Joao Afonso and Jose Pascoal and Konstantine Tsotsos and Mira Leung and Mirko Schmidt and Onur Guleryuz and Sameh Khamis and Vladimir Tankovitch and Sean Fanello and Shahram Izadi and Christoph Rhemann},
   doi = {10.1145/3272127.3275041},
   issn = {0730-0301},
   issue = {6},
   journal = {ACM Transactions on Graphics},
   month = {12},
   pages = {1-19},
   title = {Depth from motion for smartphone AR},
   volume = {37},
   year = {2018},
}
@article{Azuma1997,
   abstract = {<p>This paper surveys the field of augmented reality (AR), in which 3D virtual objects are integrated into a 3D real environment in real time. It describes the medical, manufacturing, visualization, path planning, entertainment, and military applications that have been explored. This paper describes the characteristics of augmented reality systems, including a detailed discussion of the tradeoffs between optical and video blending approaches. Registration and sensing errors are two of the biggest problems in building effective augmented reality systems, so this paper summarizes current efforts to overcome these problems. Future directions and areas requiring further research are discussed. This survey provides a starting point for anyone interested in researching or using augmented reality.</p>},
   author = {Ronald T. Azuma},
   doi = {10.1162/pres.1997.6.4.355},
   issn = {1054-7460},
   issue = {4},
   journal = {Presence: Teleoperators and Virtual Environments},
   month = {8},
   pages = {355-385},
   title = {A Survey of Augmented Reality},
   volume = {6},
   year = {1997},
}
@book_section{Leach2018,
   author = {Matthew Leach and Steve Maddock and Dawn Hadley and Carolyn Butterworth and John Moreland and Gareth Dean and Ralph Mackinder and Kacper Pach and Nick Bax and Michaela Mckone and Dan Fleetwood},
   doi = {10.1007/978-3-030-01790-3_13},
   pages = {213-229},
   title = {Recreating Sheffieldâs Medieval Castle In Situ using Outdoor AugmentedÂ Reality},
   year = {2018},
}
@article{Zandbergen2011,
   abstract = {<p>Utilizing both Assisted GPS (A-GPS) techniques and new high-sensitivity embedded GPS hardware, mobile phones are now able to achieve positioning in harsh environments such as urban canyons and indoor locations where older embedded GPS chips could not. This paper presents an empirical analysis of the positional accuracy of location data gathered using a high-sensitivity GPS-enabled mobile phone. The performance of the mobile phone is compared to that of regular recreational grade GPS receivers. Availability of valid GPS position fixes on the mobile phones tested was consistently close to 100% both outdoors and indoors. During static outdoor testing, positions provided by the mobile phones revealed a median horizontal error of between 5Â·0 and 8Â·5 m, substantially larger than those for regular autonomous GPS units by a factor of 2 to 3. Horizontal errors during static indoor testing were larger compared to outdoors, but the difference in accuracy between mobile phones and regular GPS receivers was reduced. Despite the modest performance of A-GPS on mobile phones, testing under various conditions revealed that very large errors are not very common. The maximum horizontal error during outdoor testing never exceeded 30 metres and during indoor testing never exceeded 100 metres. Combined with the relatively consistent availability of valid GPS position fixes under varying conditions, the current study has confirmed the reliability of A-GPS on mobiles phones as a source of location information for a range of different LBS applications.</p>},
   author = {Paul A. Zandbergen and Sean J. Barbeau},
   doi = {10.1017/S0373463311000051},
   issn = {0373-4633},
   issue = {3},
   journal = {Journal of Navigation},
   month = {7},
   pages = {381-399},
   title = {Positional Accuracy of Assisted GPS Data from High-Sensitivity GPS-enabled Mobile Phones},
   volume = {64},
   year = {2011},
}
@article{Panou2018,
   abstract = {<p>In this paper, we present the software architecture of a complete mobile tourist guide for cultural heritage sites located in the old town of Chania, Crete, Greece. This includes gamified components that motivate the user to traverse the suggested interest points, as well as technically challenging outdoors augmented reality (AR) visualization features. The main focus of the AR feature is to superimpose 3D models of historical buildings in their past state onto the real world, while users walk around the Venetian part of Chaniaâs city, exploring historical information in the form of text and images. We examined and tested registration and tracking mechanisms based on commercial AR frameworks in the challenging outdoor, sunny environment of a Mediterranean town, addressing relevant technical challenges. Upon visiting one of three significant monuments, a 3D model displaying the monument in its past state is visualized onto the mobile phoneâs screen at the exact location of the real-world monument, while the user is exploring the area. A location-based experience was designed and integrated into the application, enveloping the 3D model with real-world information at the same time. The users are urged to explore interest areas and unlock historical information, while earning points following a gamified experience. By combining AR technologies with location-aware and gamified elements, we aim to promote the technologically enhanced public appreciation of cultural heritage sites and showcase the cultural depth of the city of Chania.</p>},
   author = {Chris Panou and Lemonia Ragia and Despoina Dimelli and Katerina Mania},
   doi = {10.3390/ijgi7120463},
   issn = {2220-9964},
   issue = {12},
   journal = {ISPRS International Journal of Geo-Information},
   month = {11},
   pages = {463},
   title = {An Architecture for Mobile Outdoors Augmented Reality for Cultural Heritage},
   volume = {7},
   year = {2018},
}
@article{Yousif2015,
   author = {Khalid Yousif and Alireza Bab-Hadiashar and Reza Hoseinnezhad},
   doi = {10.1007/s40903-015-0032-7},
   issn = {2363-6912},
   issue = {4},
   journal = {Intelligent Industrial Systems},
   month = {12},
   pages = {289-311},
   title = {An Overview to Visual Odometry and Visual SLAM: Applications to Mobile Robotics},
   volume = {1},
   year = {2015},
}
@report{reich2020,
   author = {Andreas Reich and Linus Ehmann and Simon Storl-Schulke and Sophie Tobisch},
   city = {Furtwangen},
   institution = {Hochschule Furtwangen University},
   title = {Photogrammetric recordings of the SABA area},
   year = {2020},
}
@report{nisaba2021,
   author = {Andre Schaefer and Christian Randazzo and Felix Brunn and Franziska Schuettelbauer and Julia Adraess and Moritz Messner},
   city = {Furtwangen},
   institution = {Hochschue Furtwangen University},
   title = {NISABA: Photogrammetrische Erfassung historischer GebÃ¤udekomplexe in Villingen-Schwenningen},
   year = {2021},
}
@report{kusch2021,
   author = {Oliver Kusch and Moritz Beaugrand and Benjamin Herzberger and Matthias Muehl and Simon Loeffler and Johannes Keller and Jonas Plotzky and Jonas Nolde},
   city = {Furtwangen},
   institution = {Hochschule Furtwangen},
   title = {Kreation von 3D Modellen der alten Kaserne in Villingen-Schwenningen - Eine Analyse der Arbeitspipeline zur Erstellung von 3D Modellen mittels Pix4d},
   year = {2021},
}
@article{Leutenegger2015,
   abstract = {<p>Combining visual and inertial measurements has become popular in mobile robotics, since the two sensing modalities offer complementary characteristics that make them the ideal choice for accurate visualâinertial odometry or simultaneous localization and mapping (SLAM). While historically the problem has been addressed with filtering, advancements in visual estimation suggest that nonlinear optimization offers superior accuracy, while still tractable in complexity thanks to the sparsity of the underlying problem. Taking inspiration from these findings, we formulate a rigorously probabilistic cost function that combines reprojection errors of landmarks and inertial terms. The problem is kept tractable and thus ensuring real-time operation by limiting the optimization to a bounded window of keyframes through marginalization. Keyframes may be spaced in time by arbitrary intervals, while still related by linearized inertial terms. We present evaluation results on complementary datasets recorded with our custom-built stereo visualâinertial hardware that accurately synchronizes accelerometer and gyroscope measurements with imagery. A comparison of both a stereo and monocular version of our algorithm with and without online extrinsics estimation is shown with respect to ground truth. Furthermore, we compare the performance to an implementation of a state-of-the-art stochastic cloning sliding-window filter. This competitive reference implementation performs tightly coupled filtering-based visualâinertial odometry. While our approach declaredly demands more computation, we show its superior performance in terms of accuracy.</p>},
   author = {Stefan Leutenegger and Simon Lynen and Michael Bosse and Roland Siegwart and Paul Furgale},
   doi = {10.1177/0278364914554813},
   issn = {0278-3649},
   issue = {3},
   journal = {The International Journal of Robotics Research},
   month = {3},
   pages = {314-334},
   title = {Keyframe-based visualâinertial odometry using nonlinear optimization},
   volume = {34},
   year = {2015},
}
@book{moeller2019,
   author = {Tomas Akenine-Moeller and Eric Haines and Naty Hoffman},
   doi = {10.1201/9781315365459},
   isbn = {9781315365459},
   month = {1},
   publisher = {A K Peters/CRC Press},
   title = {Real-Time Rendering},
   year = {2019},
}
@inproceedings{coorg1997,
   author = {Satyan Coorg and Seth Teller},
   city = {New York, New York, USA},
   doi = {10.1145/253284.253312},
   isbn = {0897918843},
   journal = {Proceedings of the 1997 symposium on Interactive 3D graphics  - SI3D '97},
   pages = {83-ff.},
   publisher = {ACM Press},
   title = {Real-time occlusion culling for models with large occluders},
   year = {1997},
}
@book{nischwitz2012,
   author = {Alfred Nischwitz and Max Fischer and Peter HaberÃ¤cker and Gudrun Socher},
   city = {Wiesbaden},
   doi = {10.1007/978-3-8348-8323-0},
   isbn = {978-3-8348-1304-6},
   publisher = {Vieweg+Teubner Verlag},
   title = {Computergrafik und Bildverarbeitung},
   year = {2012},
}
@article{mitchell2005,
   author = {Jason L. Mitchell},
   journal = {ATI Research Technical Report},
   pages = {121-126},
   title = {Real-Time Synthesis and Rendering of Ocean Water},
   year = {2005},
}
@article{isidoro2002,
   author = {John Isidoro and Alex Vlachos and Chirs Brennan},
   journal = {Direct3D ShaderX: Vertex and Pixel Shader Tips and Tricks, Wordware},
   title = {Rendering ocean water},
   year = {2002},
}
@online{scratchapixel,
   author = {Scratchapixel},
   title = {Rasterization: a Practical Implementation},
   year = {2022},
   url = {https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/overview-rasterization-algorithm},
}
@inproceedings{Speicher2019,
   author = {Maximilian Speicher and Brian D. Hall and Michael Nebeling},
   city = {New York, NY, USA},
   doi = {10.1145/3290605.3300767},
   isbn = {9781450359702},
   journal = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
   month = {5},
   pages = {1-15},
   publisher = {ACM},
   title = {What is Mixed Reality?},
   year = {2019},
}
@inproceedings{Milgram1995,
   author = {Paul Milgram and Haruo Takemura and Akira Utsumi and Fumio Kishino},
   doi = {10.1117/12.197321},
   editor = {Hari Das},
   month = {12},
   pages = {282-292},
   title = {Augmented reality: a class of displays on the reality-virtuality continuum},
   year = {1995},
}
@online{UnityARFoundation,
  title = {Unity Dokumentation, About AR Foundation},
  year = 2021,
  url = {https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@5.0/manual/index.html},
  urldate = {2022-07-27}
}
@book{herling2011,
   author = {Jan Herling and Wolfgang Broll},
   city = {New York, NY},
   doi = {10.1007/978-1-4614-0064-6_11},
   journal = {Handbook of Augmented Reality},
   pages = {255-272},
   publisher = {Springer New York},
   title = {Markerless Tracking for Augmented Reality},
   year = {2011},
}

@report{harris1988,
   abstract = {Consistency of image edge filtering is of prime importance for 3D interpretation of image sequences using feature tracking algorithms. To cater for image regions containing texture and isolated features, a combined corner and edge detector based on the local auto-correlation function is utilised, and it is shown to perform with good consistency on natural imagery.},
   author = {Chris Harris and Mike Stephens},
   doi = {10.1.1.434.4816},
   title = {A combined Corner and Edge Detector},
   year = {1988},
}
@inproceedings{rosten2005,
   author = {E. Rosten and T. Drummond},
   doi = {10.1109/ICCV.2005.104},
   isbn = {0-7695-2334-X},
   journal = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
   pages = {1508-1515 Vol. 2},
   publisher = {IEEE},
   title = {Fusing points and lines for high performance tracking},
   year = {2005},
}
@book{szeliski2022,
   author = {Richard Szeliski},
   city = {Cham},
   doi = {10.1007/978-3-030-34372-9},
   isbn = {978-3-030-34371-2},
   publisher = {Springer International Publishing},
   title = {Computer Vision},
   year = {2022},
}
@inproceedings{Rublee2011,
   author = {Ethan Rublee and Vincent Rabaud and Kurt Konolige and Gary Bradski},
   doi = {10.1109/ICCV.2011.6126544},
   isbn = {978-1-4577-1102-2},
   journal = {2011 International Conference on Computer Vision},
   month = {11},
   pages = {2564-2571},
   publisher = {IEEE},
   title = {ORB: An efficient alternative to SIFT or SURF},
   year = {2011},
}
@webpage{weitz2022,
   author = {Edmund Weitz},
   title = {SIFT - Scale-Invariant Feature Transform},
   url = {http://weitz.de/sift/},
   year = {2022},
   month = {August},
   day = {1},
}
@inproceedings{klein2009,
   author = {Georg Klein and David Murray},
   doi = {10.1109/ISMAR.2009.5336495},
   isbn = {978-1-4244-5390-0},
   journal = {2009 8th IEEE International Symposium on Mixed and Augmented Reality},
   month = {10},
   pages = {83-86},
   publisher = {IEEE},
   title = {Parallel Tracking and Mapping on a camera phone},
   year = {2009},
}

@online{ARCoreDokumentation,
  title = {ARCore - Fundamental concepts},
  year = 2022,
  url = {https://developers.google.com/ar/develop/},
  urldate = {2022-08-02}
}
@online{ARKit,
  title = {ARKit},
  author = {Apple},
  year = 2022,
  url = {https://developer.apple.com/augmented-reality/},
  urldate = {2022-08-09}
}

@online{ARCore,
  title = {ARCore},
  author = {Google},
  year = 2022,
  url = {https://developers.google.com/ar/},
  urldate = {2022-08-09}
}
@online{Vuforia,
  title = {Vuforia Engine},
  year = 2022,
  url = {https://www.ptc.com/en/products/vuforia/vuforia-engine},
  urldate = {2022-08-09}
}
@online{Wikitude,
  title = {Wikitude},
  year = 2022,
  url = {https://www.wikitude.com/},
  urldate = {2022-08-09}
}
@online{ARToolkit,
  title = {ARToolkit X},
  year = 2022,
  url = {http://www.artoolkitx.org/},
  urldate = {2022-08-09}
}

@article{billinghurst2015,
   author = {Mark Billinghurst and Adrian Clark and Gun Lee},
   doi = {10.1561/1100000049},
   issn = {1551-3955},
   issue = {2-3},
   journal = {Foundations and TrendsÂ® in HumanâComputer Interaction},
   pages = {73-272},
   title = {A Survey of Augmented Reality},
   volume = {8},
   year = {2015},
}
@inproceedings{kato1999,
   author = {H. Kato and M. Billinghurst},
   doi = {10.1109/IWAR.1999.803809},
   isbn = {0-7695-0359-4},
   journal = {Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)},
   pages = {85-94},
   publisher = {IEEE Comput. Soc},
   title = {Marker tracking and HMD calibration for a video-based augmented reality conferencing system},
   year = {1999},
}
@online{PYPLStatista2022,
  title = {Die beliebtesten Programmiersprachen weltweit laut PYPL-Index im August 20222},
  year = 2022,
  publisher = Statista,
  url = {https://de.statista.com/statistik/daten/studie/678732/umfrage/beliebteste-programmiersprachen-weltweit-laut-pypl-index/},
  urldate = {2022-08-09}
}
}
@article{breen1995,
   author = {David E. Breen and Eric Rose and Ross T. Whitaker},
   journal = {European Computer-Industry Research Centre GmbH (Forschungszentrum)},
   title = {Interactive Occlusion and Collision of Real and Virtual Objects in Augmented Reality},
   year = {1995},
}
@InProceedings{flynn2019,
author = {Flynn, John and Broxton, Michael and Debevec, Paul and DuVall, Matthew and Fyffe, Graham and Overbeck, Ryan and Snavely, Noah and Tucker, Richard},
title = {DeepView: View Synthesis With Learned Gradient Descent},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}
@InProceedings{shah2012,  
author={Shah, Manisah Mohd and Arshad, Haslina and Sulaiman, Riza},  
booktitle={2012 8th International Conference on Information Science and Digital Content Technology (ICIDT2012)},   
title={Occlusion in augmented reality},   
year={2012},  
volume={2},  
number={},  
pages={372-378},  
doi={}
}
@InProceedings{LeGendre2019,
author = {LeGendre, Chloe and Ma, Wan-Chun and Fyffe, Graham and Flynn, John and Charbonnel, Laurent and Busch, Jay and Debevec, Paul},
title = {DeepLight: Learning Illumination for Unconstrained Mobile Mixed Reality},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}
@inproceedings{green2003,
  title={Spherical Harmonic Lighting: The Gritty Details},
  author={Robert Green},
  year={2003}
}